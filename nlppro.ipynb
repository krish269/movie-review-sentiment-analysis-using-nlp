{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOf7bD/vtDcPXZ12y27M9IM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krish269/movie-review-sentiment-analysis-using-nlp/blob/main/nlppro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JtYLCwJqFeK",
        "outputId": "5776a75e-0ad2-4c9b-9b05-a8164eabd9b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensuring NLTK data is available...\n",
            "NLTK data is ready.\n",
            "\n",
            "Loading data from IMDB Dataset.csv...\n",
            "Data loaded successfully.\n",
            "Dataset preview:\n",
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "\n",
            "Dataset info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     50000 non-null  object\n",
            " 1   sentiment  50000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 781.4+ KB\n",
            "\n",
            "Preprocessing all reviews in the dataset (this may take a while)...\n",
            "Preprocessing complete.\n",
            "Dataset with cleaned reviews:\n",
            "                                              review sentiment  \\\n",
            "0  One of the other reviewers has mentioned that ...  positive   \n",
            "1  A wonderful little production. <br /><br />The...  positive   \n",
            "2  I thought this was a wonderful way to spend ti...  positive   \n",
            "3  Basically there's a family where a little boy ...  negative   \n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
            "\n",
            "                                      cleaned_review  \n",
            "0  one reviewer mentioned watching oz episode hoo...  \n",
            "1  wonderful little production filming technique ...  \n",
            "2  thought wonderful way spend time hot summer we...  \n",
            "3  basically family little boy jake think zombie ...  \n",
            "4  petter mattei love time money visually stunnin...  \n",
            "\n",
            "Creating TF-IDF features...\n",
            "TF-IDF features created.\n",
            "\n",
            "Data split into training and testing sets.\n",
            "Training set shape: (40000, 5000)\n",
            "Testing set shape: (10000, 5000)\n",
            "\n",
            "Training the model...\n",
            "Model training complete.\n",
            "\n",
            "Evaluating the model...\n",
            "Accuracy: 0.8909\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.90      0.88      0.89      5000\n",
            "    Positive       0.88      0.90      0.89      5000\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4397  603]\n",
            " [ 488 4512]]\n",
            "\n",
            "Saving model and vectorizer to disk...\n",
            "Files saved successfully.\n",
            "\n",
            "--- Interactive Sentiment Prediction ---\n",
            "Enter a movie review to predict its sentiment.\n",
            "Type 'quit' or 'exit' to stop.\n",
            "\n",
            "Enter your review: the movie was great , happy to watch it\n",
            "Predicted Sentiment: POSITIVE\n",
            "\n",
            "Enter your review: it was a disaster\n",
            "Predicted Sentiment: NEGATIVE\n",
            "\n",
            "Enter your review: quit\n",
            "Exiting interactive prediction.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import sys\n",
        "\n",
        "def download_nltk_data():\n",
        "    \"\"\"\n",
        "    Downloads all necessary NLTK data packs.\n",
        "    The downloader will skip any packages that are already up-to-date.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Ensuring NLTK data is available...\")\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        nltk.download('wordnet', quiet=True)\n",
        "        nltk.download('punkt_tab', quiet=True) # Download punkt_tab\n",
        "        print(\"NLTK data is ready.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during NLTK data download: {e}\", file=sys.stderr)\n",
        "        print(\"Please check your internet connection and try again.\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "def load_data(filepath):\n",
        "    \"\"\"\n",
        "    Loads the movie review dataset from a CSV file.\n",
        "    Validates the presence of 'review' and 'sentiment' columns.\n",
        "    \"\"\"\n",
        "    print(f\"\\nLoading data from {filepath}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(filepath, encoding='utf-8')\n",
        "        print(\"Data loaded successfully.\")\n",
        "\n",
        "        required_columns = ['review', 'sentiment']\n",
        "        if not all(col in df.columns for col in required_columns):\n",
        "            print(f\"Error: The CSV file must contain the columns: {required_columns}\", file=sys.stderr)\n",
        "            return None\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"Error: The CSV file is empty.\", file=sys.stderr)\n",
        "            return None\n",
        "\n",
        "        print(\"Dataset preview:\")\n",
        "        print(df.head())\n",
        "        print(\"\\nDataset info:\")\n",
        "        df.info()\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{filepath}' was not found.\", file=sys.stderr)\n",
        "        print(\"Please make sure the 'IMDB Dataset.csv' file is in the same directory as the script.\", file=sys.stderr)\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the file: {e}\", file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    text = re.sub(re.compile('<.*?>'), '', text)\n",
        "\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 1]\n",
        "\n",
        "    return \" \".join(cleaned_tokens)\n",
        "\n",
        "def create_tfidf_vectorizer(corpus):\n",
        "    \"\"\"\n",
        "    Creates and fits a TF-IDF vectorizer on the given text corpus.\n",
        "    \"\"\"\n",
        "    print(\"\\nCreating TF-IDF features...\")\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X = vectorizer.fit_transform(corpus).toarray()\n",
        "    print(\"TF-IDF features created.\")\n",
        "    return X, vectorizer\n",
        "\n",
        "def train_model(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains a Logistic Regression model.\n",
        "    \"\"\"\n",
        "    print(\"\\nTraining the model...\")\n",
        "\n",
        "    model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"Model training complete.\")\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance and prints a report.\n",
        "    \"\"\"\n",
        "    print(\"\\nEvaluating the model...\")\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(cm)\n",
        "\n",
        "def predict_sentiment(review, vectorizer, model):\n",
        "\n",
        "    processed_review = preprocess_text(review)\n",
        "\n",
        "    review_vector = vectorizer.transform([processed_review]).toarray()\n",
        "\n",
        "    prediction = model.predict(review_vector)\n",
        "\n",
        "    sentiment = 'Positive' if prediction[0] == 1 else 'Negative'\n",
        "\n",
        "    print(f\"Predicted Sentiment: {sentiment.upper()}\")\n",
        "    return sentiment\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    download_nltk_data()\n",
        "\n",
        "    df = load_data('IMDB Dataset.csv')\n",
        "\n",
        "    if df is None:\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"\\nPreprocessing all reviews in the dataset (this may take a while)...\")\n",
        "    df['cleaned_review'] = df['review'].apply(preprocess_text)\n",
        "    print(\"Preprocessing complete.\")\n",
        "    print(\"Dataset with cleaned reviews:\")\n",
        "    print(df.head())\n",
        "\n",
        "    X_features, tfidf_vectorizer = create_tfidf_vectorizer(df['cleaned_review'])\n",
        "    y_labels = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_features,\n",
        "        y_labels,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=y_labels\n",
        "    )\n",
        "    print(f\"\\nData split into training and testing sets.\")\n",
        "    print(f\"Training set shape: {X_train.shape}\")\n",
        "    print(f\"Testing set shape: {X_test.shape}\")\n",
        "\n",
        "\n",
        "    sentiment_model = train_model(X_train, y_train)\n",
        "\n",
        "    evaluate_model(sentiment_model, X_test, y_test)\n",
        "\n",
        "    print(\"\\nSaving model and vectorizer to disk...\")\n",
        "    joblib.dump(sentiment_model, 'sentiment_model.pkl')\n",
        "    joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
        "    print(\"Files saved successfully.\")\n",
        "\n",
        "    print(\"\\n--- Interactive Sentiment Prediction ---\")\n",
        "    print(\"Enter a movie review to predict its sentiment.\")\n",
        "    print(\"Type 'quit' or 'exit' to stop.\")\n",
        "\n",
        "    while True:\n",
        "        user_review = input(\"\\nEnter your review: \")\n",
        "        if user_review.lower() in ['quit', 'exit']:\n",
        "            print(\"Exiting interactive prediction.\")\n",
        "            break\n",
        "\n",
        "        if not user_review.strip():\n",
        "            print(\"Please enter a review.\")\n",
        "            continue\n",
        "\n",
        "        predict_sentiment(\n",
        "            user_review,\n",
        "            tfidf_vectorizer,\n",
        "            sentiment_model\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "import sys\n",
        "\n",
        "def download_nltk_data():\n",
        "    \"\"\"\n",
        "    Downloads all necessary NLTK data packs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Ensuring NLTK data is available...\")\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        nltk.download('wordnet', quiet=True)\n",
        "        print(\"NLTK data is ready.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during NLTK data download: {e}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "def load_data(filepath):\n",
        "    \"\"\"\n",
        "    Loads the movie review dataset from a CSV file.\n",
        "    \"\"\"\n",
        "    print(f\"\\nLoading data from {filepath}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(filepath, encoding='utf-8')\n",
        "        print(\"Data loaded successfully.\")\n",
        "        if 'review' not in df.columns or 'sentiment' not in df.columns:\n",
        "            print(\"Error: CSV must contain 'review' and 'sentiment' columns.\", file=sys.stderr)\n",
        "            return None\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{filepath}' was not found.\", file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and preprocesses a single piece of text.\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = re.sub(re.compile('<.*?>'), '', text)\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    text = text.lower()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 1]\n",
        "    return \" \".join(cleaned_tokens)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    download_nltk_data()\n",
        "\n",
        "    df = load_data('IMDB Dataset.csv')\n",
        "    if df is None:\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"\\nPreprocessing all reviews (this may take a while)...\")\n",
        "    df['cleaned_review'] = df['review'].apply(preprocess_text)\n",
        "    print(\"Preprocessing complete.\")\n",
        "\n",
        "    print(\"\\nCreating TF-IDF features...\")\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_features = tfidf_vectorizer.fit_transform(df['cleaned_review']).toarray()\n",
        "    y_labels = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
        "    print(\"TF-IDF features created.\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_features, y_labels, test_size=0.2, random_state=42, stratify=y_labels\n",
        "    )\n",
        "    print(\"\\nData split into training and testing sets.\")\n",
        "\n",
        "    print(\"\\nTraining the model...\")\n",
        "    model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"Model training complete.\")\n",
        "\n",
        "    print(\"\\nEvaluating the model...\")\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
        "\n",
        "    print(\"\\nSaving model and vectorizer to disk...\")\n",
        "    joblib.dump(model, 'sentiment_model.pkl')\n",
        "    joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
        "    print(\"Files 'sentiment_model.pkl' and 'tfidf_vectorizer.pkl' saved successfully.\")\n",
        "    print(\"\\n--- Training complete. You can now run the prediction script. ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jQMoiogzlUb",
        "outputId": "5e6673eb-a270-4f9a-c5ea-ecb8cc7ea562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensuring NLTK data is available...\n",
            "NLTK data is ready.\n",
            "\n",
            "Loading data from IMDB Dataset.csv...\n",
            "Data loaded successfully.\n",
            "\n",
            "Preprocessing all reviews (this may take a while)...\n",
            "Preprocessing complete.\n",
            "\n",
            "Creating TF-IDF features...\n",
            "TF-IDF features created.\n",
            "\n",
            "Data split into training and testing sets.\n",
            "\n",
            "Training the model...\n",
            "Model training complete.\n",
            "\n",
            "Evaluating the model...\n",
            "Model Accuracy: 0.8909\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.90      0.88      0.89      5000\n",
            "    Positive       0.88      0.90      0.89      5000\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n",
            "\n",
            "Saving model and vectorizer to disk...\n",
            "Files 'sentiment_model.pkl' and 'tfidf_vectorizer.pkl' saved successfully.\n",
            "\n",
            "--- Training complete. You can now run the prediction script. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOCK 2: Load Model and Predict\n",
        "# This script loads the pre-trained model and vectorizer and provides an\n",
        "# interactive prompt to predict the sentiment of new movie reviews.\n",
        "# Run this script anytime after the training script has been executed successfully.\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import joblib\n",
        "import sys\n",
        "\n",
        "# --- 1. NLTK Data Download (needed for preprocessing) ---\n",
        "def download_nltk_data():\n",
        "    \"\"\"\n",
        "    Ensures NLTK data is available for the preprocessing function.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "        nltk.data.find('corpora/stopwords')\n",
        "        nltk.data.find('corpora/wordnet')\n",
        "    except LookupError:\n",
        "        print(\"Downloading necessary NLTK data...\")\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# --- 2. Text Preprocessing Function ---\n",
        "# This function MUST be identical to the one used during training.\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and preprocesses a single piece of text.\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = re.sub(re.compile('<.*?>'), '', text)\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    text = text.lower()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 1]\n",
        "    return \" \".join(cleaned_tokens)\n",
        "\n",
        "# --- 3. Prediction Pipeline ---\n",
        "def predict_sentiment(review, vectorizer, model):\n",
        "    \"\"\"\n",
        "    Takes a single movie review and predicts its sentiment.\n",
        "    \"\"\"\n",
        "    processed_review = preprocess_text(review)\n",
        "    review_vector = vectorizer.transform([processed_review]).toarray()\n",
        "    prediction = model.predict(review_vector)\n",
        "    sentiment = 'Positive' if prediction[0] == 1 else 'Negative'\n",
        "    print(f\"Predicted Sentiment: {sentiment.upper()}\")\n",
        "\n",
        "# --- Main Prediction Execution ---\n",
        "if __name__ == '__main__':\n",
        "    # Ensure NLTK data is ready\n",
        "    download_nltk_data()\n",
        "\n",
        "    # Load the saved model and vectorizer\n",
        "    try:\n",
        "        print(\"Loading trained model and vectorizer...\")\n",
        "        model = joblib.load('sentiment_model.pkl')\n",
        "        vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
        "        print(\"Model and vectorizer loaded successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\nError: Model or vectorizer files not found.\", file=sys.stderr)\n",
        "        print(\"Please run the 'train_model.py' script first to train and save the model.\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Interactive prediction loop\n",
        "    print(\"\\n--- Interactive Sentiment Prediction ---\")\n",
        "    print(\"Enter a movie review to predict its sentiment.\")\n",
        "    print(\"Type 'quit' or 'exit' to stop.\")\n",
        "\n",
        "    while True:\n",
        "        user_review = input(\"\\nEnter your review: \")\n",
        "        if user_review.lower() in ['quit', 'exit']:\n",
        "            print(\"Exiting interactive prediction.\")\n",
        "            break\n",
        "\n",
        "        if not user_review.strip():\n",
        "            print(\"Please enter a review.\")\n",
        "            continue\n",
        "\n",
        "        predict_sentiment(user_review, vectorizer, model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eiGqgLozqXi",
        "outputId": "c42f602f-b460-4f35-b65e-8d96ab2cf457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading necessary NLTK data...\n",
            "Loading trained model and vectorizer...\n",
            "Model and vectorizer loaded successfully.\n",
            "\n",
            "--- Interactive Sentiment Prediction ---\n",
            "Enter a movie review to predict its sentiment.\n",
            "Type 'quit' or 'exit' to stop.\n",
            "\n",
            "Enter your review: nice\n",
            "Predicted Sentiment: POSITIVE\n",
            "\n",
            "Enter your review: quit\n",
            "Exiting interactive prediction.\n"
          ]
        }
      ]
    }
  ]
}