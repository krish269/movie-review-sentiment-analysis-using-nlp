{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krish269/movie-review-sentiment-analysis-using-nlp/blob/main/nlppro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JtYLCwJqFeK",
        "outputId": "f6bf30c1-41e3-479a-a20e-7fb721b15953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensuring NLTK data is available...\n",
            "NLTK data is ready.\n",
            "\n",
            "Loading data from IMDB Dataset.csv...\n",
            "Data loaded successfully.\n",
            "Dataset preview:\n",
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "\n",
            "Dataset info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     50000 non-null  object\n",
            " 1   sentiment  50000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 781.4+ KB\n",
            "\n",
            "Preprocessing all reviews in the dataset (this may take a while)...\n",
            "Preprocessing complete.\n",
            "Dataset with cleaned reviews:\n",
            "                                              review sentiment  \\\n",
            "0  One of the other reviewers has mentioned that ...  positive   \n",
            "1  A wonderful little production. <br /><br />The...  positive   \n",
            "2  I thought this was a wonderful way to spend ti...  positive   \n",
            "3  Basically there's a family where a little boy ...  negative   \n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
            "\n",
            "                                      cleaned_review  \n",
            "0  one reviewer mentioned watching oz episode hoo...  \n",
            "1  wonderful little production filming technique ...  \n",
            "2  thought wonderful way spend time hot summer we...  \n",
            "3  basically family little boy jake think zombie ...  \n",
            "4  petter mattei love time money visually stunnin...  \n",
            "\n",
            "Creating TF-IDF features...\n",
            "TF-IDF features created.\n",
            "\n",
            "Data split into training and testing sets.\n",
            "Training set shape: (40000, 5000)\n",
            "Testing set shape: (10000, 5000)\n",
            "\n",
            "Training the model...\n",
            "Model training complete.\n",
            "\n",
            "Evaluating the model...\n",
            "Accuracy: 0.8909\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.90      0.88      0.89      5000\n",
            "    Positive       0.88      0.90      0.89      5000\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4397  603]\n",
            " [ 488 4512]]\n",
            "\n",
            "Saving model and vectorizer to disk...\n",
            "Files saved successfully.\n",
            "\n",
            "--- Interactive Sentiment Prediction ---\n",
            "Enter a movie review to predict its sentiment.\n",
            "Type 'quit' or 'exit' to stop.\n",
            "\n",
            "Enter your review: This movie was a disgrace\n",
            "Predicted Sentiment: NEGATIVE\n",
            "\n",
            "Enter your review: exit\n",
            "Exiting interactive prediction.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import sys\n",
        "\n",
        "def download_nltk_data():\n",
        "    \"\"\"\n",
        "    Downloads all necessary NLTK data packs.\n",
        "    The downloader will skip any packages that are already up-to-date.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Ensuring NLTK data is available...\")\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        nltk.download('wordnet', quiet=True)\n",
        "        nltk.download('punkt_tab', quiet=True) # Download punkt_tab\n",
        "        print(\"NLTK data is ready.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during NLTK data download: {e}\", file=sys.stderr)\n",
        "        print(\"Please check your internet connection and try again.\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "def load_data(filepath):\n",
        "    \"\"\"\n",
        "    Loads the movie review dataset from a CSV file.\n",
        "    Validates the presence of 'review' and 'sentiment' columns.\n",
        "    \"\"\"\n",
        "    print(f\"\\nLoading data from {filepath}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(filepath, encoding='utf-8')\n",
        "        print(\"Data loaded successfully.\")\n",
        "\n",
        "        required_columns = ['review', 'sentiment']\n",
        "        if not all(col in df.columns for col in required_columns):\n",
        "            print(f\"Error: The CSV file must contain the columns: {required_columns}\", file=sys.stderr)\n",
        "            return None\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"Error: The CSV file is empty.\", file=sys.stderr)\n",
        "            return None\n",
        "\n",
        "        print(\"Dataset preview:\")\n",
        "        print(df.head())\n",
        "        print(\"\\nDataset info:\")\n",
        "        df.info()\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{filepath}' was not found.\", file=sys.stderr)\n",
        "        print(\"Please make sure the 'IMDB Dataset.csv' file is in the same directory as the script.\", file=sys.stderr)\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the file: {e}\", file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    text = re.sub(re.compile('<.*?>'), '', text)\n",
        "\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 1]\n",
        "\n",
        "    return \" \".join(cleaned_tokens)\n",
        "\n",
        "def create_tfidf_vectorizer(corpus):\n",
        "    \"\"\"\n",
        "    Creates and fits a TF-IDF vectorizer on the given text corpus.\n",
        "    \"\"\"\n",
        "    print(\"\\nCreating TF-IDF features...\")\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X = vectorizer.fit_transform(corpus).toarray()\n",
        "    print(\"TF-IDF features created.\")\n",
        "    return X, vectorizer\n",
        "\n",
        "def train_model(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains a Logistic Regression model.\n",
        "    \"\"\"\n",
        "    print(\"\\nTraining the model...\")\n",
        "\n",
        "    model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"Model training complete.\")\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance and prints a report.\n",
        "    \"\"\"\n",
        "    print(\"\\nEvaluating the model...\")\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(cm)\n",
        "\n",
        "def predict_sentiment(review, vectorizer, model):\n",
        "\n",
        "    processed_review = preprocess_text(review)\n",
        "\n",
        "    review_vector = vectorizer.transform([processed_review]).toarray()\n",
        "\n",
        "    prediction = model.predict(review_vector)\n",
        "\n",
        "    sentiment = 'Positive' if prediction[0] == 1 else 'Negative'\n",
        "\n",
        "    print(f\"Predicted Sentiment: {sentiment.upper()}\")\n",
        "    return sentiment\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    download_nltk_data()\n",
        "\n",
        "    df = load_data('IMDB Dataset.csv')\n",
        "\n",
        "    if df is None:\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"\\nPreprocessing all reviews in the dataset (this may take a while)...\")\n",
        "    df['cleaned_review'] = df['review'].apply(preprocess_text)\n",
        "    print(\"Preprocessing complete.\")\n",
        "    print(\"Dataset with cleaned reviews:\")\n",
        "    print(df.head())\n",
        "\n",
        "    X_features, tfidf_vectorizer = create_tfidf_vectorizer(df['cleaned_review'])\n",
        "    y_labels = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_features,\n",
        "        y_labels,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=y_labels\n",
        "    )\n",
        "    print(f\"\\nData split into training and testing sets.\")\n",
        "    print(f\"Training set shape: {X_train.shape}\")\n",
        "    print(f\"Testing set shape: {X_test.shape}\")\n",
        "\n",
        "\n",
        "    sentiment_model = train_model(X_train, y_train)\n",
        "\n",
        "    evaluate_model(sentiment_model, X_test, y_test)\n",
        "\n",
        "    print(\"\\nSaving model and vectorizer to disk...\")\n",
        "    joblib.dump(sentiment_model, 'sentiment_model.pkl')\n",
        "    joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
        "    print(\"Files saved successfully.\")\n",
        "\n",
        "    print(\"\\n--- Interactive Sentiment Prediction ---\")\n",
        "    print(\"Enter a movie review to predict its sentiment.\")\n",
        "    print(\"Type 'quit' or 'exit' to stop.\")\n",
        "\n",
        "    while True:\n",
        "        user_review = input(\"\\nEnter your review: \")\n",
        "        if user_review.lower() in ['quit', 'exit']:\n",
        "            print(\"Exiting interactive prediction.\")\n",
        "            break\n",
        "\n",
        "        if not user_review.strip():\n",
        "            print(\"Please enter a review.\")\n",
        "            continue\n",
        "\n",
        "        predict_sentiment(\n",
        "            user_review,\n",
        "            tfidf_vectorizer,\n",
        "            sentiment_model\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "import sys\n",
        "\n",
        "def download_nltk_data():\n",
        "    \"\"\"\n",
        "    Downloads all necessary NLTK data packs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Ensuring NLTK data is available...\")\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        nltk.download('wordnet', quiet=True)\n",
        "        print(\"NLTK data is ready.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during NLTK data download: {e}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "def load_data(filepath):\n",
        "    \"\"\"\n",
        "    Loads the movie review dataset from a CSV file.\n",
        "    \"\"\"\n",
        "    print(f\"\\nLoading data from {filepath}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(filepath, encoding='utf-8')\n",
        "        print(\"Data loaded successfully.\")\n",
        "        if 'review' not in df.columns or 'sentiment' not in df.columns:\n",
        "            print(\"Error: CSV must contain 'review' and 'sentiment' columns.\", file=sys.stderr)\n",
        "            return None\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{filepath}' was not found.\", file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and preprocesses a single piece of text.\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = re.sub(re.compile('<.*?>'), '', text)\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    text = text.lower()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 1]\n",
        "    return \" \".join(cleaned_tokens)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    download_nltk_data()\n",
        "\n",
        "    df = load_data('IMDB Dataset.csv')\n",
        "    if df is None:\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"\\nPreprocessing all reviews (this may take a while)...\")\n",
        "    df['cleaned_review'] = df['review'].apply(preprocess_text)\n",
        "    print(\"Preprocessing complete.\")\n",
        "\n",
        "    print(\"\\nCreating TF-IDF features...\")\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_features = tfidf_vectorizer.fit_transform(df['cleaned_review']).toarray()\n",
        "    y_labels = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
        "    print(\"TF-IDF features created.\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_features, y_labels, test_size=0.2, random_state=42, stratify=y_labels\n",
        "    )\n",
        "    print(\"\\nData split into training and testing sets.\")\n",
        "\n",
        "    print(\"\\nTraining the model...\")\n",
        "    model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"Model training complete.\")\n",
        "\n",
        "    print(\"\\nEvaluating the model...\")\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
        "\n",
        "    print(\"\\nSaving model and vectorizer to disk...\")\n",
        "    joblib.dump(model, 'sentiment_model.pkl')\n",
        "    joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
        "    print(\"Files 'sentiment_model.pkl' and 'tfidf_vectorizer.pkl' saved successfully.\")\n",
        "    print(\"\\n--- Training complete. You can now run the prediction script. ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jQMoiogzlUb",
        "outputId": "0461a310-c877-4433-a0bb-c0ed22d7fe66"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensuring NLTK data is available...\n",
            "NLTK data is ready.\n",
            "\n",
            "Loading data from IMDB Dataset.csv...\n",
            "Data loaded successfully.\n",
            "\n",
            "Preprocessing all reviews (this may take a while)...\n",
            "Preprocessing complete.\n",
            "\n",
            "Creating TF-IDF features...\n",
            "TF-IDF features created.\n",
            "\n",
            "Data split into training and testing sets.\n",
            "\n",
            "Training the model...\n",
            "Model training complete.\n",
            "\n",
            "Evaluating the model...\n",
            "Model Accuracy: 0.8909\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.90      0.88      0.89      5000\n",
            "    Positive       0.88      0.90      0.89      5000\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n",
            "\n",
            "Saving model and vectorizer to disk...\n",
            "Files 'sentiment_model.pkl' and 'tfidf_vectorizer.pkl' saved successfully.\n",
            "\n",
            "--- Training complete. You can now run the prediction script. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pandas nltk scikit-learn joblib pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibppmp-8a5GU",
        "outputId": "7709157e-7173-4ef4-ba68-5793c53f3239"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.50.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.4.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.6.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
            "Downloading streamlit-1.50.0-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.4.0-py3-none-any.whl (25 kB)\n",
            "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyngrok, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 pyngrok-7.4.0 streamlit-1.50.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "# This is the main script for the Streamlit web application.\n",
        "# To run this app, save the code as 'app.py' and run the following\n",
        "# command in your terminal (after activating your virtual environment):\n",
        "# streamlit run app.py\n",
        "\n",
        "import streamlit as st\n",
        "import joblib\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import os\n",
        "import ssl\n",
        "\n",
        "# --- Page Configuration ---\n",
        "st.set_page_config(\n",
        "    page_title=\"Movie Review Sentiment Analyzer\",\n",
        "    page_icon=\"🎬\",\n",
        "    layout=\"centered\",\n",
        "    initial_sidebar_state=\"auto\",\n",
        ")\n",
        "\n",
        "# --- NLTK Data Download ---\n",
        "# This function is cached to run only once.\n",
        "@st.cache_resource\n",
        "def download_nltk_data():\n",
        "    \"\"\"\n",
        "    Downloads all necessary NLTK data packs safely.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Workaround for SSL certificate verification issue\n",
        "        _create_unverified_https_context = ssl._create_unverified_context\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    else:\n",
        "        ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "    # Download required packages\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "    print(\"NLTK data downloaded successfully.\")\n",
        "\n",
        "# Call the function to ensure data is available\n",
        "download_nltk_data()\n",
        "\n",
        "# --- Load Model and Vectorizer ---\n",
        "# The models are loaded once and cached for efficiency.\n",
        "@st.cache_resource\n",
        "def load_model_and_vectorizer():\n",
        "    \"\"\"\n",
        "    Loads the pre-trained sentiment analysis model and TF-IDF vectorizer.\n",
        "    Returns the model and vectorizer, or None if files are not found.\n",
        "    \"\"\"\n",
        "    model_path = 'sentiment_model.pkl'\n",
        "    vectorizer_path = 'tfidf_vectorizer.pkl'\n",
        "\n",
        "    if not os.path.exists(model_path) or not os.path.exists(vectorizer_path):\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        model = joblib.load(model_path)\n",
        "        vectorizer = joblib.load(vectorizer_path)\n",
        "        return model, vectorizer\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading model files: {e}\")\n",
        "        return None, None\n",
        "\n",
        "model, vectorizer = load_model_and_vectorizer()\n",
        "\n",
        "# --- Text Preprocessing Function ---\n",
        "# This function must be identical to the one used during training.\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and preprocesses a single piece of text for prediction.\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = re.sub(re.compile('<.*?>'), '', text)  # Remove HTML tags\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)       # Keep only letters\n",
        "    text = text.lower()                         # Convert to lowercase\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Lemmatize and remove stopwords\n",
        "    cleaned_tokens = [\n",
        "        lemmatizer.lemmatize(word) for word in tokens\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return \" \".join(cleaned_tokens)\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "\n",
        "# Main Title\n",
        "st.title(\"🎬 Movie Review Sentiment Analyzer\")\n",
        "st.markdown(\"Enter a movie review below to determine if it's positive or negative. The model was trained on the IMDB dataset.\")\n",
        "\n",
        "# Check if model files are loaded\n",
        "if model is None or vectorizer is None:\n",
        "    st.error(\n",
        "        \"**Model files not found!** 😟\\n\\n\"\n",
        "        \"Please run the `train_model.py` script first to train the model \"\n",
        "        \"and generate the necessary `.pkl` files.\"\n",
        "    )\n",
        "else:\n",
        "    # User Input Area\n",
        "    st.subheader(\"Enter Your Movie Review\")\n",
        "    user_input = st.text_area(\n",
        "        \"Type or paste your review here...\",\n",
        "        height=150,\n",
        "        placeholder=\"e.g., 'The movie was absolutely fantastic! The acting was superb and the plot was gripping.'\"\n",
        "    )\n",
        "\n",
        "    # Analyze Button\n",
        "    if st.button(\"Analyze Sentiment\", type=\"primary\"):\n",
        "        if user_input.strip():\n",
        "            # Preprocess the input\n",
        "            processed_input = preprocess_text(user_input)\n",
        "\n",
        "            # Vectorize the input\n",
        "            input_vector = vectorizer.transform([processed_input]).toarray()\n",
        "\n",
        "            # Make a prediction\n",
        "            prediction = model.predict(input_vector)\n",
        "            probability = model.predict_proba(input_vector)\n",
        "\n",
        "            sentiment = 'Positive' if prediction[0] == 1 else 'Negative'\n",
        "            confidence = probability[0][prediction[0]]\n",
        "\n",
        "            # Display the result\n",
        "            st.subheader(\"Analysis Result\")\n",
        "            if sentiment == 'Positive':\n",
        "                st.success(f\"**Sentiment: Positive** 👍 ({confidence:.2%} confidence)\")\n",
        "                st.balloons()\n",
        "            else:\n",
        "                st.error(f\"**Sentiment: Negative** 👎 ({confidence:.2%} confidence)\")\n",
        "        else:\n",
        "            st.warning(\"Please enter a review to analyze.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eiGqgLozqXi",
        "outputId": "1175d7b1-48d0-484e-fb3d-92e56113c77f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# --- IMPORTANT ---\n",
        "# Get your authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "# and paste it here:\n",
        "authtoken = \"33eDdoJHJx3L5S5ybXgZGov1478_2DSbx5G87gTHnbusbg1c8\"\n",
        "ngrok.set_auth_token(authtoken)\n",
        "\n",
        "# Terminate open tunnels if any\n",
        "ngrok.kill()\n",
        "\n",
        "# Start streamlit in background\n",
        "!nohup streamlit run app.py --server.port 8501 &\n",
        "\n",
        "# Open a tunnel to the streamlit port\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Click the following URL to view your Streamlit app: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0nCckuWbK2u",
        "outputId": "f613677a-a359-4fa6-f631-4930f4295063"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "Click the following URL to view your Streamlit app: NgrokTunnel: \"https://fencelike-ali-untributarily.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}